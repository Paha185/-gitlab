![image](https://github.com/user-attachments/assets/da17a5b0-682c-48f0-8bcb-da4686d53c02)
В данном эксперементе мы видин что при обучении количество ошибок на обучающей выборке снизилась, н опри этом точность на тестовой выборки осталась не удовлетворительна 
Для улучшения точности были произведены попытки изменить конфигурацию модели увеличив колличество слоев LSTM и менялись количество нейронов на входе и выходе между слоями
![image](https://github.com/user-attachments/assets/101ff18a-3633-487b-83aa-df75181cf717)
На данных графиках представленна 2 эксперемента в сразными конфигурациями модели.
В 1 случае был слишком сложная модель с большим колличеством слоев из-за чего не охотно обучалась. Точность на тестовой выборке составила чуть меньше 60 %, что является не очень хорошим результатом.
В 2 случае была выбрана стондартная модель с 2 слоями и количеством нейронов но перед работай была произведена нормализация данных, что позврлило модели лучше обучаться.
![image](https://github.com/user-attachments/assets/d4529ec3-0358-4fb5-ad90-b0e7d3b4383e)
В последнем эксперементе была произведена нормализация и выбрано 3 слоя сети LSTM.
Максимальный результат был достигнут 92% на тестовой выборке. Ошибка на тренировочной и тестовой выборке также равномерно уменьшалась, что говорит о том что преобучения не происходило.
